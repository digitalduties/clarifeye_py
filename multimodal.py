# -*- coding: utf-8 -*-
"""multimodal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gvC6ikyABpuHvLfcpP4QrnXQNkNaKvKX

# Week 3: Exploring Multi-modalities
Author: Pat Pascual - AI First Season 2
"""

# Update OpenAI to latest version
!pip install --upgrade openai

import openai
openai.api_key = ""

"""## Mode 1 - **Text-to-Text**: Text Chat with System Prompting"""

model = "gpt-4o"
struct = [{"role": "system", "content": "You are a helpful AI tutor."}]

print("\U0001F9E0 Text Chat Assistant is ready! Type 'exit' or 'quit' to stop.\n")
while True:
    user_message = input("User: ")
    if user_message.lower() in ["exit", "quit"]:
        print("\U0001F44B Goodbye!")
        break
    struct.append({"role": "user", "content": user_message})
    response = openai.chat.completions.create(model=model, messages=struct)
    assistant_reply = response.choices[0].message.content.strip()
    print("Assistant:", assistant_reply)
    struct.append({"role": "assistant", "content": assistant_reply})

"""## Mode 2 - **Text-to-Image**: AI Image Generation with DALL¬∑E 3"""

print("\nüé® AI Image Generator - Type your prompt or 'exit' to quit")
while True:
    prompt = input("Prompt: ")
    if prompt.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = openai.images.generate(
        model="dall-e-3",
        prompt=prompt,
        n=1,
        size="1024x1024"
    )
    img_url = response.data[0].url
    print("Generated Image URL:", img_url)

"""## Mode 3 - **Image-to-Text**: Image Interpretation with GPT-4o"""

import base64
from google.colab import files, output, widgets
import openai
import ipywidgets as ipy
from IPython.display import display, clear_output

# UI elements
upload_btn = ipy.FileUpload(accept='image/*', multiple=False)
analyze_btn = ipy.Button(description='Analyze Image', button_style='success')
clear_btn = ipy.Button(description='Clear', button_style='danger')
output_area = ipy.Output()

# Display widgets
display(ipy.VBox([upload_btn, ipy.HBox([analyze_btn, clear_btn]), output_area]))

# Analyze image handler
def analyze_image(btn):
    with output_area:
        clear_output()
        if not upload_btn.value:
            print("‚ö†Ô∏è Please upload an image first.")
            return

        # Show loading indicator
        print("‚è≥ Analyzing image, please wait...")

    # Read and encode image
    filename = next(iter(upload_btn.value))
    content = upload_btn.value[filename]['content']
    b64_img = base64.b64encode(content).decode('utf-8')

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": "You are an agent that scans throught the list of ingredients in a cosmetic product. The list usually starts with Ingredients:. Please do not interpret any other images that do not have text in it, in particualr, if it does not list ingredients. For each ingredient, give a brief information on its effect on the skin. Say for example, if an ingredient does not have a direct effect on the skin whether safe or harmful, no need to list (e.g. some are just solvents or thickeners)"},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }]
        )
        with output_area:
            clear_output()
            print("üß† GPT-4o Analysis:", response.choices[0].message.content.strip())
    except Exception as e:
        with output_area:
            clear_output()
            print("‚ùå Error during analysis:", e)

# Clear handler
def clear_all(btn):
    upload_btn.value.clear()
    upload_btn._counter = 0  # reset internal counter
    with output_area:
        clear_output()
        print("üßπ Cleared. Upload a new image to try again.")

# Bind buttons
analyze_btn.on_click(analyze_image)
clear_btn.on_click(clear_all)

import base64
from google.colab import files, output, widgets
import openai
import ipywidgets as ipy
from IPython.display import display, clear_output

# UI elements
upload_btn = ipy.FileUpload(accept='image/*', multiple=False)
prompt_input = ipy.Textarea(
    value="What do you see in this image?",
    description='Prompt:',
    layout=ipy.Layout(width='100%', height='80px')
)
analyze_btn = ipy.Button(description='Analyze Image', button_style='success')
clear_btn = ipy.Button(description='Clear', button_style='danger')
output_area = ipy.Output()

# Display widgets
display(ipy.VBox([
    upload_btn,
    prompt_input,
    ipy.HBox([analyze_btn, clear_btn]),
    output_area
]))

# Analyze image handler
def analyze_image(btn):
    with output_area:
        clear_output()
        if not upload_btn.value:
            print("‚ö†Ô∏è Please upload an image first.")
            return

        print("‚è≥ Analyzing image, please wait...")

    # Read and encode image
    filename = next(iter(upload_btn.value))
    content = upload_btn.value[filename]['content']
    b64_img = base64.b64encode(content).decode('utf-8')
    user_prompt = "You are an agent that scans through ingredients in a cosmetic product"

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }]
        )
        with output_area:
            clear_output()
            print("üß† GPT-4o Analysis:", response.choices[0].message.content.strip())
    except Exception as e:
        with output_area:
            clear_output()
            print("‚ùå Error during analysis:", e)

# Clear handler
def clear_all(btn):
    upload_btn.value.clear()
    upload_btn._counter = 0
    prompt_input.value = "What do you see in this image?"
    with output_area:
        clear_output()
        print("üßπ Cleared. Upload a new image to try again.")

# Bind buttons
analyze_btn.on_click(analyze_image)
clear_btn.on_click(clear_all)

import base64
from google.colab import files, output, widgets
import openai
import ipywidgets as ipy
from IPython.display import display, clear_output

# Setup UI elements
upload_btn = ipy.FileUpload(accept='image/*', multiple=False)
prompt_box = ipy.Text(
    value='What do you see in this image?',
    placeholder='Enter your prompt about the image...',
    description='Prompt:',
    layout=ipy.Layout(width='100%')
)
analyze_btn = ipy.Button(description='Analyze Image', button_style='success')
clear_btn = ipy.Button(description='Clear', button_style='danger')
output_area = ipy.Output()

# Display widgets
display(ipy.VBox([upload_btn, prompt_box, ipy.HBox([analyze_btn, clear_btn]), output_area]))

# Analyze image handler
def analyze_image(btn):
    with output_area:
        clear_output()
        if not upload_btn.value:
            print("‚ö†Ô∏è Please upload an image first.")
            return

        # Show loading message
        print("‚è≥ Analyzing image, please wait...")

    # Read image and encode to base64
    filename = next(iter(upload_btn.value))
    content = upload_btn.value[filename]['content']
    b64_img = base64.b64encode(content).decode('utf-8')

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_box.value},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }]
        )
        with output_area:
            clear_output()
            print("üß† GPT-4o Analysis:", response.choices[0].message.content.strip())
    except Exception as e:
        with output_area:
            clear_output()
            print("‚ùå Error during analysis:", e)

# Clear handler
def clear_all(btn):
    upload_btn.value.clear()
    upload_btn._counter = 0  # reset internal counter
    prompt_box.value = 'What do you see in this image?'
    with output_area:
        clear_output()
        print("üßπ Cleared. Upload a new image to try again.")

# Bind buttons
analyze_btn.on_click(analyze_image)
clear_btn.on_click(clear_all)

"""## Mode 4 - **Audio-to-Text**: Audio Transcription with Whisper"""

import os, base64, openai
import ipywidgets as ipy
from IPython.display import display, clear_output, Audio, HTML, Javascript
from google.colab import output as colab_output

# Widgets
upload_btn = ipy.FileUpload(accept='.mp3,.m4a,.wav,.flac', multiple=False)
record_btn = ipy.Button(description='üéôÔ∏è Record', button_style='primary')
stop_btn = ipy.Button(description='‚èπÔ∏è Stop', button_style='warning', disabled=True)
transcribe_btn = ipy.Button(description='üìù Transcribe', button_style='success', disabled=True)
clear_btn = ipy.Button(description='üßπ Clear', button_style='danger')
output_area = ipy.Output()

# Display UI
display(ipy.VBox([
    ipy.Label("üîä Audio Transcription - Upload or Record"),
    upload_btn,
    ipy.HBox([record_btn, stop_btn, transcribe_btn, clear_btn]),
    output_area
]))

# Audio waveform recorder JavaScript
RECORD_HTML = """
<canvas id="waveform" width="300" height="60" style="background: #f0f0f0; border-radius: 5px;"></canvas>
<script>
const canvas = document.getElementById('waveform');
const ctx = canvas.getContext('2d');
let mediaRecorder, audioChunks = [], audioStream, analyser, dataArray, animationId;

async function startRecording() {
  audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const audioCtx = new AudioContext();
  const source = audioCtx.createMediaStreamSource(audioStream);
  analyser = audioCtx.createAnalyser();
  source.connect(analyser);
  analyser.fftSize = 256;
  const bufferLength = analyser.frequencyBinCount;
  dataArray = new Uint8Array(bufferLength);

  function draw() {
    animationId = requestAnimationFrame(draw);
    analyser.getByteTimeDomainData(dataArray);
    ctx.fillStyle = '#f0f0f0';
    ctx.fillRect(0, 0, canvas.width, canvas.height);
    ctx.lineWidth = 2;
    ctx.strokeStyle = '#007bff';
    ctx.beginPath();
    const sliceWidth = canvas.width / bufferLength;
    let x = 0;
    for (let i = 0; i < bufferLength; i++) {
      const v = dataArray[i] / 128.0;
      const y = v * canvas.height / 2;
      i === 0 ? ctx.moveTo(x, y) : ctx.lineTo(x, y);
      x += sliceWidth;
    }
    ctx.lineTo(canvas.width, canvas.height / 2);
    ctx.stroke();
  }

  draw();
  audioChunks = [];
  mediaRecorder = new MediaRecorder(audioStream);
  mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
  mediaRecorder.onstop = async () => {
    cancelAnimationFrame(animationId);
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    audioStream.getTracks().forEach(track => track.stop());

    const blob = new Blob(audioChunks, { type: 'audio/webm' });
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64Audio = reader.result.split(',')[1];
      google.colab.kernel.invokeFunction('notebook.onAudioCaptured', [base64Audio], {});
    };
    reader.readAsDataURL(blob);
  };
  mediaRecorder.start();
}
function stopRecording() {
  if (mediaRecorder && mediaRecorder.state !== "inactive") {
    mediaRecorder.stop();
  }
}
startRecording();
</script>
"""

# Callback for JS audio capture
def on_audio_captured(b64_audio):
    raw = base64.b64decode(b64_audio)
    with open("recording.webm", "wb") as f:
        f.write(raw)
    with output_area:
        clear_output()
        print("‚úÖ Recording complete. You can now transcribe.")
        display(Audio("recording.webm"))
    transcribe_btn.disabled = False

colab_output.register_callback('notebook.onAudioCaptured', on_audio_captured)

# Handlers
def start_recording(btn):
    with output_area:
        clear_output()
        print("üé§ Recording... Speak into the mic.")
        display(HTML(RECORD_HTML))
    record_btn.disabled = True
    stop_btn.disabled = False
    transcribe_btn.disabled = True

def stop_recording(btn):
    display(Javascript("stopRecording();"))
    record_btn.disabled = False
    stop_btn.disabled = True

def transcribe_audio(btn):
    with output_area:
        clear_output()
        print("‚è≥ Transcribing, please wait...")
        file_path = None

        # Upload flow
        if upload_btn.value:
            filename = next(iter(upload_btn.value))
            content = upload_btn.value[filename]['content']
            file_path = filename
            with open(file_path, "wb") as f:
                f.write(content)
        # Recorded audio
        elif os.path.exists("recording.webm"):
            file_path = "recording.webm"
        else:
            print("‚ö†Ô∏è No audio file found.")
            return

        try:
            with open(file_path, "rb") as audio_file:
                transcript = openai.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file
                )
            print("üìù Transcription:\n", transcript.text)
        except Exception as e:
            print("‚ùå Error during transcription:", e)
        finally:
            if file_path and file_path != "recording.webm" and os.path.exists(file_path):
                os.remove(file_path)

def clear_all(btn):
    upload_btn.value.clear()
    upload_btn._counter = 0
    transcribe_btn.disabled = True
    record_btn.disabled = False
    stop_btn.disabled = True
    for f in ["recording.webm"]:
        if os.path.exists(f):
            os.remove(f)
    with output_area:
        clear_output()
        print("üßπ Cleared. Upload or record new audio.")

# Bind Events
record_btn.on_click(start_recording)
stop_btn.on_click(stop_recording)
transcribe_btn.on_click(transcribe_audio)
clear_btn.on_click(clear_all)

"""## Mode 5 - **Text-to-Audio**: Text-to-Speech with TTS"""

import openai
import ipywidgets as ipy
from IPython.display import display, Audio, clear_output
import os

# UI elements
text_input = ipy.Textarea(
    value="",
    placeholder="Enter text to synthesize...",
    description="Text:",
    layout=ipy.Layout(width='100%', height='100px')
)
generate_btn = ipy.Button(description="Generate", button_style="success")
download_btn = ipy.Button(description="Download", button_style="info", disabled=True)
clear_btn = ipy.Button(description="Clear", button_style="danger")
output_area = ipy.Output()

# Display widgets
display(ipy.VBox([
    ipy.Label("üó£Ô∏è Text-to-Speech"),
    text_input,
    ipy.HBox([generate_btn, clear_btn, download_btn]),
    output_area
]))

# File name for the output
output_file = "output.mp3"

# Generate button handler
def generate_tts(btn):
    with output_area:
        clear_output()
        if not text_input.value.strip():
            print("‚ö†Ô∏è Please enter some text.")
            return
        print("üîÑ Generating, please wait...")

        try:
            response = openai.audio.speech.create(
                model="tts-1",
                voice="nova",
                input=text_input.value
            )
            with open(output_file, "wb") as f:
                f.write(response.content)
            print("‚úÖ Audio generated.")
            display(Audio(output_file))
            download_btn.disabled = False
        except Exception as e:
            print("‚ùå Error during generation:", e)

# Download button handler
def download_audio(btn):
    from google.colab import files
    if os.path.exists(output_file):
        files.download(output_file)

# Clear button handler
def clear_all(btn):
    text_input.value = ""
    with output_area:
        clear_output()
        print("üßπ Cleared. Enter new text to synthesize.")
    download_btn.disabled = True
    if os.path.exists(output_file):
        os.remove(output_file)

# Bind events
generate_btn.on_click(generate_tts)
clear_btn.on_click(clear_all)
download_btn.on_click(download_audio)

"""## Mode 6 - **Audio-to-Audio**: Full Voice Interaction (Ask with audio, respond with voice)"""

import base64, openai, os
from IPython.display import display, HTML, Javascript, Audio, clear_output
import ipywidgets as ipy
from google.colab import output as colab_output

# Container for UI
record_button = ipy.Button(description="üéôÔ∏è Start Recording", button_style="primary")
stop_button = ipy.Button(description="‚èπÔ∏è Stop", button_style="warning", disabled=True)
generate_button = ipy.Button(description="üß† Generate Response", button_style="success", disabled=True)
clear_button = ipy.Button(description="üßπ Clear", button_style="danger")
output_area = ipy.Output()

display(ipy.VBox([ipy.HBox([record_button, stop_button, generate_button, clear_button]), output_area]))

# JavaScript + HTML recorder with waveform
RECORD_HTML = """
<div id="recorder">
  <canvas id="waveform" width="300" height="60" style="background: #f0f0f0; border-radius: 5px;"></canvas>
  <script>
    const canvas = document.getElementById('waveform');
    const ctx = canvas.getContext('2d');
    let animationId;
    let mediaRecorder, audioChunks = [], audioStream, analyser, dataArray;

    async function startRecording() {
      audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(audioStream);
      analyser = audioContext.createAnalyser();
      source.connect(analyser);
      analyser.fftSize = 256;
      const bufferLength = analyser.frequencyBinCount;
      dataArray = new Uint8Array(bufferLength);

      function draw() {
        animationId = requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);
        ctx.fillStyle = '#f0f0f0';
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.lineWidth = 2;
        ctx.strokeStyle = '#007bff';
        ctx.beginPath();
        const sliceWidth = canvas.width * 1.0 / bufferLength;
        let x = 0;
        for(let i = 0; i < bufferLength; i++) {
          let v = dataArray[i] / 128.0;
          let y = v * canvas.height / 2;
          if(i === 0) ctx.moveTo(x, y);
          else ctx.lineTo(x, y);
          x += sliceWidth;
        }
        ctx.lineTo(canvas.width, canvas.height/2);
        ctx.stroke();
      }

      draw();
      audioChunks = [];
      mediaRecorder = new MediaRecorder(audioStream);
      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = async () => {
        cancelAnimationFrame(animationId);
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        audioStream.getTracks().forEach(track => track.stop());

        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        const reader = new FileReader();
        reader.onloadend = () => {
          const base64Audio = reader.result.split(',')[1];
          google.colab.kernel.invokeFunction('notebook.onAudioCaptured', [base64Audio], {});
        };
        reader.readAsDataURL(blob);
      };
      mediaRecorder.start();
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
      }
    }

    window.startRecording = startRecording;
    window.stopRecording = stopRecording;
  </script>
</div>
"""

# Audio capture callback
def on_audio_captured(b64_audio):
    raw = base64.b64decode(b64_audio)
    with open("user_audio.webm", "wb") as f:
        f.write(raw)
    with output_area:
        clear_output()
        print("‚úÖ Recording complete. You can now generate a response.")
        display(Audio("user_audio.webm"))
    generate_button.disabled = False

colab_output.register_callback('notebook.onAudioCaptured', on_audio_captured)

# Button actions
def start_recording(btn):
    with output_area:
        clear_output()
        print("üéôÔ∏è Recording... Speak now!")
        display(HTML(RECORD_HTML))
        display(Javascript("startRecording();"))
    record_button.disabled = True
    stop_button.disabled = False
    generate_button.disabled = True

def stop_recording(btn):
    display(Javascript("stopRecording();"))
    record_button.disabled = False
    stop_button.disabled = True

def generate_response(btn):
    with output_area:
        clear_output()
        if not os.path.exists("user_audio.webm"):
            print("‚ö†Ô∏è No audio file found.")
            return
        print("‚è≥ Transcribing...")
        with open("user_audio.webm", "rb") as f:
            transcript = openai.audio.transcriptions.create(model="whisper-1", file=f)
        question = transcript.text
        print("üó£Ô∏è You said:", question)

        print("üí¨ GPT-4o responding...")
        chat = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": question}]
        )
        answer = chat.choices[0].message.content.strip()
        print("ü§ñ GPT-4o:", answer)

        print("üîä Generating speech...")
        tts = openai.audio.speech.create(model="tts-1", voice="nova", input=answer)
        with open("gpt_response.mp3", "wb") as f:
            f.write(tts.content)
        display(Audio("gpt_response.mp3"))

def clear_all(btn):
    for file in ["user_audio.webm", "gpt_response.mp3"]:
        if os.path.exists(file):
            os.remove(file)
    with output_area:
        clear_output()
        print("üßπ Cleared. Ready to start a new voice interaction.")
    record_button.disabled = False
    stop_button.disabled = True
    generate_button.disabled = True

# Button bindings
record_button.on_click(start_recording)
stop_button.on_click(stop_recording)
generate_button.on_click(generate_response)
clear_button.on_click(clear_all)

"""## Mode 7 - File to Text: Upload PDF/XLSX/WORD to Text Chat"""

!pip install streamlit python-docx PyPDF2 pandas openai fpdf

import os, openai, docx, PyPDF2, pandas as pd
import ipywidgets as ipy
from IPython.display import display, clear_output
from fpdf import FPDF

# Set your OpenAI key
openai.api_key = ""

# Widgets
upload_btn = ipy.FileUpload(accept='.pdf,.docx,.xlsx', multiple=False)
generate_btn = ipy.Button(description='üí¨ Generate', button_style='success', disabled=True)
clear_btn = ipy.Button(description='üßπ Clear', button_style='danger')
instruction_input = ipy.Textarea(placeholder="Type your instruction or question here...", layout=ipy.Layout(width="100%", height="100px"))
output_area = ipy.Output()

# App state
doc_text = {"content": ""}

# Display UI
display(ipy.VBox([
    ipy.Label("üìÑ Document Q&A - Upload and Ask"),
    upload_btn,
    instruction_input,
    ipy.HBox([generate_btn, clear_btn]),
    output_area
]))

# Utilities
def extract_text_from_upload(file_info):
    filename = next(iter(file_info))
    content = file_info[filename]['content']
    ext = os.path.splitext(filename)[1].lower()

    # Save temp file
    path = "/tmp/" + filename
    with open(path, "wb") as f:
        f.write(content)

    text = ""
    if ext == ".pdf":
        with open(path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
    elif ext == ".docx":
        doc = docx.Document(path)
        for para in doc.paragraphs:
            text += para.text + "\n"
    elif ext == ".xlsx":
        df = pd.read_excel(path)
        text = df.to_string()
    else:
        text = "Unsupported file format."

    return text

# Handlers
def handle_upload(change):
    if upload_btn.value:
        with output_area:
            clear_output()
            print("üì§ File uploaded. Extracting text...")
        try:
            doc_text["content"] = extract_text_from_upload(upload_btn.value)
            with output_area:
                clear_output()
                print("‚úÖ Extracted text preview:\n")
                print(doc_text["content"][:500])
            generate_btn.disabled = False
        except Exception as e:
            with output_area:
                clear_output()
                print("‚ùå Error reading file:", e)

def handle_generate(btn):
    if not doc_text["content"]:
        with output_area:
            clear_output()
            print("‚ö†Ô∏è Please upload a document first.")
        return
    if not instruction_input.value.strip():
        with output_area:
            clear_output()
            print("‚ö†Ô∏è Please enter an instruction or question.")
        return

    with output_area:
        clear_output()
        print("‚è≥ Generating response...")
    try:
        struct = [
            {"role": "system", "content": "You are a helpful assistant that works with uploaded documents."},
            {"role": "user", "content": f"Here is the document content:\n{doc_text['content'][:3000]}\n\n{instruction_input.value}"}
        ]
        response = openai.chat.completions.create(model="gpt-4o", messages=struct)
        result = response.choices[0].message.content.strip()
        with output_area:
            clear_output()
            print("ü§ñ AI Response:\n", result)

        # Save to PDF
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)
        for line in result.split("\n"):
            pdf.multi_cell(0, 10, line)
        pdf_path = "/tmp/response_output.pdf"
        pdf.output(pdf_path)

        with open(pdf_path, "rb") as f:
            from IPython.display import FileLink
            display(FileLink(pdf_path, result_html_prefix="üìÑ Download: "))

    except Exception as e:
        with output_area:
            clear_output()
            print("‚ùå Error generating response:", e)

def handle_clear(btn):
    upload_btn.value.clear()
    upload_btn._counter = 0
    instruction_input.value = ""
    generate_btn.disabled = True
    doc_text["content"] = ""
    with output_area:
        clear_output()
        print("üßπ Cleared. You can upload a new document.")

# Bind events
upload_btn.observe(handle_upload, names='value')
generate_btn.on_click(handle_generate)
clear_btn.on_click(handle_clear)